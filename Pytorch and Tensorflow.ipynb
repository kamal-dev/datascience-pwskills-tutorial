{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is an activation function in the context of artificial neural networks?\n",
    "\n",
    "An activation function determines the output of a neuron given its input. It introduces non-linearity into the network, enabling it to learn complex patterns.\n",
    "\n",
    "Q2. What are some common types of activation functions used in neural networks?\n",
    "\n",
    "Sigmoid, ReLU (Rectified Linear Unit), Leaky ReLU, Softmax, Tanh (Hyperbolic Tangent), etc.\n",
    "\n",
    "Q3. How do activation functions affect the training process and performance of a neural network?\n",
    "\n",
    "- Activation functions affect the network's ability to learn and generalize.\n",
    "- They control the range and distribution of neuron outputs, impacting convergence speed and model expressiveness.\n",
    "\n",
    "Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?\n",
    "\n",
    "Sigmoid squashes input values between 0 and 1, useful for binary classification tasks.\n",
    "Advantages: Smooth gradient, output in [0, 1] suitable for probability interpretation.\n",
    "Disadvantages: Susceptible to vanishing gradient problem, outputs not centered around zero causing convergence issues in deeper networks.\n",
    "\n",
    "Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?\n",
    "\n",
    "ReLU outputs the input directly if positive, otherwise outputs zero.\n",
    "Unlike sigmoid, ReLU doesn't saturate for positive inputs, helping mitigate the vanishing gradient problem.\n",
    "\n",
    "Q6. What are the benefits of using the ReLU activation function over the sigmoid function?\n",
    "\n",
    "Faster convergence: ReLU enables faster training due to its non-saturating nature and avoids vanishing gradient issues.\n",
    "Sparsity: ReLU can lead to sparsity in activations, reducing computational complexity.\n",
    "\n",
    "Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem.\n",
    "\n",
    "Leaky ReLU allows a small, non-zero gradient when the input is negative, preventing dead neurons and addressing the vanishing gradient problem.\n",
    "\n",
    "Q8. What is the purpose of the softmax activation function? When is it commonly used?\n",
    "\n",
    "Softmax converts raw scores into probabilities, useful for multi-class classification where each class is mutually exclusive.\n",
    "It ensures that the output probabilities sum up to 1, facilitating interpretation as class probabilities.\n",
    "\n",
    "Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?\n",
    "\n",
    "Tanh squashes input values between -1 and 1, similar to sigmoid but centered around zero.\n",
    "Compared to sigmoid, tanh has a larger output range, helping alleviate the vanishing gradient problem to some extent."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
