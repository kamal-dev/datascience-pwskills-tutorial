{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "695f8e44",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "KNN (K-Nearest Neighbors) is a supervised machine learning algorithm used for both classification and regression tasks.\n",
    "It classifies a data point based on the majority class of its nearest neighbors (for classification) or predicts its value based on the average of its nearest neighbors' values (for regression).\n",
    "\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "The value of K in KNN is typically chosen using techniques like cross-validation or grid search.\n",
    "A smaller value of K may lead to a more flexible model with higher variance and lower bias, while a larger value of K may result in a smoother decision boundary with lower variance and higher bias.\n",
    "\n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "KNN classifier predicts the class label of a data point based on the majority class of its nearest neighbors.\n",
    "KNN regressor predicts the numerical value of a data point based on the average (or median) value of its nearest neighbors.\n",
    "\n",
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "The performance of KNN can be measured using evaluation metrics such as accuracy, precision, recall, F1-score (for classification), or RMSE (Root Mean Squared Error), MAE (Mean Absolute Error) (for regression).\n",
    "Cross-validation techniques like k-fold cross-validation can also be used to estimate the generalization performance of the model.\n",
    "\n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "The curse of dimensionality refers to the phenomenon where the performance of KNN deteriorates as the number of features (dimensions) increases.\n",
    "In high-dimensional spaces, the distance between data points becomes less meaningful, leading to poor performance of the algorithm.\n",
    "\n",
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "One approach to handle missing values in KNN is to impute them by replacing them with the mean, median, or mode of the feature values of neighboring data points.\n",
    "Another approach is to use distance-weighted imputation, where missing values are estimated based on the weighted average of neighboring data points' values.\n",
    "\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n",
    "\n",
    "The performance of KNN classifier and regressor depends on the nature of the problem and the characteristics of the dataset.\n",
    "KNN classifier is suitable for classification tasks where decision boundaries are nonlinear and the dataset is not too large.\n",
    "KNN regressor is suitable for regression tasks where the relationship between input features and output values is not linear and there are no strong assumptions about the data distribution.\n",
    "\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,and how can these be addressed?\n",
    "\n",
    "- Strengths: Simple and easy to implement, effective for non-linear data, handles multi-class problems well.\n",
    "- Weaknesses: Computationally expensive for large datasets, sensitive to irrelevant features and outliers, requires careful selection of K and distance metric.\n",
    "- Addressing weaknesses: Feature scaling, dimensionality reduction techniques, and careful selection of K and distance metric can help improve the performance of KNN.\n",
    "\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "- Euclidean distance measures the straight-line distance between two points in Euclidean space.\n",
    "- Manhattan distance (also known as city block distance or L1 norm) measures the sum of the absolute differences between corresponding coordinates of two points.\n",
    "\n",
    "Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "Feature scaling is important in KNN because it ensures that all features contribute equally to the distance calculations.\n",
    "Without feature scaling, features with larger scales may dominate the distance metric, leading to biased results.\n",
    "Common scaling techniques include Min-Max scaling (scaling features to a range of [0, 1]) and standardization (scaling features to have mean 0 and standard deviation 1)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
